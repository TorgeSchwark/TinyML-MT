{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d269ec4",
   "metadata": {},
   "source": [
    "# Testing our YOLO checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d0d7622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"/data22/stu235269/TinyML-MT/model_checkpoints/yolo_classifier1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5355485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path(\"/data22/stu235269/TinyML-MT/huggingface/inference\")\n",
    "selected_paths = list(image_folder.glob(\"*.jpg\")) + list(image_folder.glob(\"*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb977038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x512 (no detections), 1.4ms\n",
      "1: 512x512 1 avocado, 1.4ms\n",
      "2: 512x512 2 tomato sauces, 1.4ms\n",
      "3: 512x512 1 cucumber, 4 tomato sauces, 1.4ms\n",
      "4: 512x512 1 avocado, 2 fruit teas, 2 tomato sauces, 1.4ms\n",
      "5: 512x512 2 fruit teas, 1 tomato sauce, 1.4ms\n",
      "6: 512x512 3 avocados, 2 fruit teas, 2 tomato sauces, 1.4ms\n",
      "7: 512x512 2 fruit teas, 1.4ms\n",
      "8: 512x512 2 fruit teas, 1.4ms\n",
      "9: 512x512 3 fruit teas, 1 tomato sauce, 1.4ms\n",
      "10: 512x512 1 avocado, 1.4ms\n",
      "11: 512x512 1 apple, 1 cucumber, 1.4ms\n",
      "12: 512x512 1 apple, 1 lemon, 1.4ms\n",
      "13: 512x512 1 apple, 1 lemon, 1.4ms\n",
      "14: 512x512 (no detections), 1.4ms\n",
      "15: 512x512 1 avocado, 1 fruit tea, 1.4ms\n",
      "16: 512x512 1 tomato sauce, 1.4ms\n",
      "17: 512x512 1 fruit tea, 2 tomato sauces, 1.4ms\n",
      "18: 512x512 1 apple, 1 avocado, 1 cucumber, 4 fruit teas, 1 lemon, 1.4ms\n",
      "19: 512x512 1 avocado, 1 banana, 2 lemons, 1 tomato sauce, 1.4ms\n",
      "20: 512x512 1 apple, 2 cucumbers, 1 tomato sauce, 1.4ms\n",
      "21: 512x512 2 avocados, 1 fruit tea, 4 tomato sauces, 1.4ms\n",
      "22: 512x512 1 lemon, 4 tomato sauces, 1.4ms\n",
      "23: 512x512 1 lemon, 5 tomato sauces, 1.4ms\n",
      "Speed: 3.0ms preprocess, 1.4ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(\n",
    "    selected_paths,\n",
    "    imgsz=512,\n",
    "    save=False,   # don't auto-save, we'll do it manually\n",
    "    stream=False, # get a list of results\n",
    "    verbose=True,  # optional: see output logs\n",
    "    visualize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7001f546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: predicted_images_yolo1/IMG_8693_pred.jpg\n",
      "Saved: predicted_images_yolo1/IMG_8694_pred.jpg\n",
      "Saved: predicted_images_yolo1/IMG_8695_pred.jpg\n",
      "Saved: predicted_images_yolo1/IMG_8696_pred.jpg\n",
      "Saved: predicted_images_yolo1/IMG_8697_pred.jpg\n",
      "Saved: predicted_images_yolo1/IMG_8698_pred.jpg\n",
      "Saved: predicted_images_yolo1/IMG_8699_pred.jpg\n",
      "Saved: predicted_images_yolo1/IMG_8700_pred.jpg\n",
      "Saved: predicted_images_yolo1/IMG_8701_pred.jpg\n",
      "Saved: predicted_images_yolo1/IMG_8702_pred.jpg\n",
      "Saved: predicted_images_yolo1/IMG_8703_pred.jpg\n",
      "Saved: predicted_images_yolo1/IMG_8704_pred.jpg\n",
      "Saved: predicted_images_yolo1/image_1230_pred.jpg\n",
      "Saved: predicted_images_yolo1/image_1231_pred.jpg\n",
      "Saved: predicted_images_yolo1/image_1651_pred.jpg\n",
      "Saved: predicted_images_yolo1/image_1674_pred.jpg\n",
      "Saved: predicted_images_yolo1/generated_image_17_pred.jpg\n",
      "Saved: predicted_images_yolo1/generated_image_18_pred.jpg\n",
      "Saved: predicted_images_yolo1/generated_image_19_pred.jpg\n",
      "Saved: predicted_images_yolo1/generated_image_20_pred.jpg\n",
      "Saved: predicted_images_yolo1/generated_image_21_pred.jpg\n",
      "Saved: predicted_images_yolo1/generated_image_22_pred.jpg\n",
      "Saved: predicted_images_yolo1/generated_image_23_pred.jpg\n",
      "Saved: predicted_images_yolo1/generated_image_24_pred.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "output_dir = Path(\"predicted_images_yolo1\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for i, result in enumerate(preds):\n",
    "    # result.plot() returns a NumPy array ready for cv2.imwrite (BGR)\n",
    "    img_bgr = result.plot()\n",
    "\n",
    "    # Use the original file name or create a numbered name\n",
    "    original_path = selected_paths[i]\n",
    "    out_path = output_dir / f\"{original_path.stem}_pred.jpg\"\n",
    "\n",
    "    cv2.imwrite(str(out_path), img_bgr)\n",
    "    print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692b123",
   "metadata": {},
   "source": [
    "## Test the COREML File Conversion\n",
    "The App is worse than the original checkpoints of images fro m the phone so we want to see if nms=True hurst the accuracy or if the App has a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f440f874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " - image\n",
      " - iouThreshold\n",
      " - confidenceThreshold\n",
      "Input shape: (640, 640, 3)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Model prediction is only supported on macOS version 10.13 or later.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(mlmodel\u001b[38;5;241m.\u001b[39minput_description)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Run prediction\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmlmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_array\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Print outputs\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputs:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data22/stu235269/miniconda3/envs/basket/lib/python3.10/site-packages/coremltools/models/model.py:780\u001b[0m, in \u001b[0;36mMLModel.predict\u001b[0;34m(self, data, state)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:   \u001b[38;5;66;03m# Error case\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _macos_version() \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m13\u001b[39m):\n\u001b[0;32m--> 780\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    781\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel prediction is only supported on macOS version 10.13 or later.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    782\u001b[0m         )\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _MLModelProxy:\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load CoreML.framework. Cannot make predictions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Model prediction is only supported on macOS version 10.13 or later."
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Path to your model.mlpackage\n",
    "model_path = \"/data22/stu235269/TinyML-MT/model_checkpoints/1-shot.mlpackage\"\n",
    "\n",
    "# Load the MLModel\n",
    "mlmodel = ct.models.MLModel(model_path)\n",
    "\n",
    "# Inspect input names\n",
    "print(\"Inputs:\")\n",
    "for input in mlmodel.input_description:\n",
    "    print(\" -\", input)\n",
    "\n",
    "# Load and preprocess your image\n",
    "image = Image.open(\"/data22/stu235269/TinyML-MT/huggingface/inference/IMG_8700.jpg\").convert(\"RGB\")\n",
    "image = image.resize((640, 640))   # or do letterbox here if needed\n",
    "\n",
    "# Convert to numpy array\n",
    "input_array = np.array(image).astype(np.float32) / 255.0\n",
    "\n",
    "# Check input shape (some models expect NCHW, others NHWC)\n",
    "print(\"Input shape:\", input_array.shape)\n",
    "\n",
    "# Depending on the model, you might need to transpose (HWC->CHW)\n",
    "# e.g., input_array = np.transpose(input_array, (2,0,1))\n",
    "\n",
    "# Get input name (e.g., \"image\")\n",
    "input_name = list(mlmodel.input_description)[0]\n",
    "\n",
    "# Run prediction\n",
    "outputs = mlmodel.predict({input_name: input_array})\n",
    "\n",
    "# Print outputs\n",
    "print(\"Outputs:\")\n",
    "for k, v in outputs.items():\n",
    "    print(k, type(v), getattr(v, \"shape\", None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basket",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
