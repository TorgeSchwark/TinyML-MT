{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "f7759347",
            "metadata": {},
            "source": [
                "# YOLO Training & Testing"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "52c43a71",
            "metadata": {},
            "source": [
                "### Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "5411391b",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import shutil"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4f799271",
            "metadata": {},
            "source": [
                "### GPUs, HF Cache"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "414198ee",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "env: CUDA_VISIBLE_DEVICES=1\n",
                        "../../.cache\n",
                        "TorgeSchwark\n",
                        "\u001b[1morgs: \u001b[0m Basket-AEye\n"
                    ]
                }
            ],
            "source": [
                "%matplotlib inline \n",
                "\n",
                "# Model\n",
                "%env CUDA_VISIBLE_DEVICES=1\n",
                "#%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32\n",
                "\n",
                "# HF Cache\n",
                "os.environ[\"HF_HOME\"] = \"../../.cache\"\n",
                "!echo $HF_HOME\n",
                "!huggingface-cli whoami"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "39be4fe6",
            "metadata": {},
            "source": [
                "### Select MVTec with specific Labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "6cbc15dd",
            "metadata": {},
            "outputs": [],
            "source": [
                "IMG_SIZE = 500\n",
                "EPOCHS = 30\n",
                "DATASET_PATH = \"../../huggingface/ai_shelf/artificial_rotated_mult_back_advanced_pipe/dataset.yaml\"\n",
                "DATASET_PATH = os.path.abspath(DATASET_PATH)\n",
                "NAME = \"artificial_created_mult_back_rotated_big_yolol_advanced_pipe\" # For WANDB (YOLO11n-COCO11-on_mvtec_train_with_augmented)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "9a9f0cab",
            "metadata": {},
            "outputs": [],
            "source": [
                "import wandb\n",
                "wandb.finish()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "07bb669e",
            "metadata": {},
            "outputs": [],
            "source": [
                "def log_class_metrics_heatmap(val_results, null_classes=[], wandb_key=\"class_metrics_heatmap\"):\n",
                "    \"\"\"\n",
                "    Erstellt eine Heatmap aus den Klassenspezifischen Metriken (Precision, Recall, F1, AP@0.5)\n",
                "    aus den val_results eines YOLOv8-Modells und loggt sie zu Weights & Biases.\n",
                "    Diese Version ist robust gegen√ºber fehlenden Klassen im Val-Set.\n",
                "    \n",
                "    Parameter:\n",
                "        val_results: Das Ergebnisobjekt von model.val()\n",
                "        wandb_key (str): Der Key unter dem das Bild bei W&B geloggt wird\n",
                "    \"\"\"\n",
                "    import numpy as np\n",
                "    import matplotlib.pyplot as plt\n",
                "    import wandb\n",
                "\n",
                "    # Klassennamen sortiert\n",
                "    names_dict = val_results.names\n",
                "    sorted_class_ids_and_names = sorted(names_dict.items())\n",
                "    print(\"sorted_class_ids_and_names \", sorted_class_ids_and_names)\n",
                "    \n",
                "    map_id_on_result_id = {}\n",
                "    count = 0\n",
                "    for i, name in sorted_class_ids_and_names:\n",
                "        if name in null_classes:\n",
                "            map_id_on_result_id[i] = None \n",
                "        else:\n",
                "            map_id_on_result_id[i] = count\n",
                "            count += 1\n",
                "        \n",
                "    names = [name for _, name in sorted_class_ids_and_names if name not in null_classes]\n",
                "    class_ids = [i for i, _ in sorted_class_ids_and_names]\n",
                "\n",
                "    # Zugriff auf Metriken\n",
                "    p = val_results.box.p if hasattr(val_results.box, 'p') else []\n",
                "    r = val_results.box.r if hasattr(val_results.box, 'r') else []\n",
                "    f1 = val_results.box.f1 if hasattr(val_results.box, 'f1') else []\n",
                "    ap = val_results.box.all_ap if hasattr(val_results.box, 'all_ap') else []\n",
                "\n",
                "    # Hilfsfunktion zum sicheren Zugriff\n",
                "    def safe_get(metric_list, idx, default=0.0):\n",
                "        return metric_list[idx] if idx < len(metric_list) else default\n",
                "\n",
                "    def safe_ap0(metric_list, idx):\n",
                "        return metric_list[idx][0] if idx < len(metric_list) and len(metric_list[idx]) > 0 else 0.0\n",
                "\n",
                "    # Metriken extrahieren pro Klasse\n",
                "    precisions = [safe_get(p, map_id_on_result_id[i]) for i in class_ids if map_id_on_result_id[i] != None]\n",
                "    recalls = [safe_get(r, map_id_on_result_id[i]) for i in class_ids if map_id_on_result_id[i] != None]\n",
                "    f1s = [safe_get(f1, map_id_on_result_id[i]) for i in class_ids if map_id_on_result_id[i] != None]\n",
                "    ap50s = [safe_ap0(ap, map_id_on_result_id[i]) for i in class_ids if map_id_on_result_id[i] != None]\n",
                "\n",
                "    metrics_matrix = np.array([\n",
                "        precisions,\n",
                "        recalls,\n",
                "        f1s,\n",
                "        ap50s\n",
                "    ])\n",
                "\n",
                "    metric_names = ['Precision', 'Recall', 'F1', 'AP@0.5']\n",
                "\n",
                "    # Heatmap erzeugen\n",
                "    fig, ax = plt.subplots(figsize=(max(8, len(names) * 0.8), 4))\n",
                "    im = ax.imshow(metrics_matrix, cmap='viridis', vmin=0, vmax=1)\n",
                "\n",
                "    ax.set_xticks(np.arange(len(names)))\n",
                "    ax.set_xticklabels(names, rotation=45, ha=\"right\")\n",
                "    ax.set_yticks(np.arange(len(metric_names)))\n",
                "    ax.set_yticklabels(metric_names)\n",
                "\n",
                "    for i in range(metrics_matrix.shape[0]):\n",
                "        for j in range(metrics_matrix.shape[1]):\n",
                "            ax.text(j, i, f\"{metrics_matrix[i, j]:.2f}\", ha=\"center\", va=\"center\",\n",
                "                    color=\"white\" if metrics_matrix[i, j] < 0.5 else \"black\")\n",
                "\n",
                "    plt.colorbar(im, ax=ax)\n",
                "    plt.title(\"Metriken pro Klasse\")\n",
                "    plt.tight_layout()\n",
                "\n",
                "    wandb.log({wandb_key: wandb.Image(fig)})\n",
                "\n",
                "    plt.close(fig)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "b2ff4a9a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtorge-schwark\u001b[0m (\u001b[33mmaats\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
                        "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
                        "E0000 00:00:1750246437.933648  327865 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
                        "E0000 00:00:1750246437.940687  327865 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "Tracking run with wandb version 0.19.4"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Run data is saved locally in <code>/data22/stu236894/GitRepos/TinyML-MT/code_training/classification-code/wandb/run-20250618_133357-qvtys6fx</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Syncing run <strong><a href='https://wandb.ai/maats/Yolo-Training/runs/qvtys6fx' target=\"_blank\">artificial_created_mult_back_rotated_big_yolol_advanced_pipe18Jun-13:33:57</a></strong> to <a href='https://wandb.ai/maats/Yolo-Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View project at <a href='https://wandb.ai/maats/Yolo-Training' target=\"_blank\">https://wandb.ai/maats/Yolo-Training</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run at <a href='https://wandb.ai/maats/Yolo-Training/runs/qvtys6fx' target=\"_blank\">https://wandb.ai/maats/Yolo-Training/runs/qvtys6fx</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "New https://pypi.org/project/ultralytics/8.3.156 available üòÉ Update with 'pip install -U ultralytics'\n",
                        "Ultralytics 8.3.129 üöÄ Python-3.10.12 torch-2.5.1+cu124 CUDA:0 (NVIDIA TITAN Xp, 12183MiB)\n",
                        "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.1, copy_paste_mode=flip, cos_lr=False, cutmix=0.3, data=/data22/stu236894/GitRepos/TinyML-MT/huggingface/ai_shelf/artificial_rotated_mult_back_advanced_pipe/dataset.yaml, degrees=180, deterministic=True, device=1, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.1, hsv_s=0.7, hsv_v=0.4, imgsz=500, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.3, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=artificial_created_mult_back_rotated_big_yolol_advanced_pipe18Jun-13:33:57, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0003, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/train, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/train/artificial_created_mult_back_rotated_big_yolol_advanced_pipe18Jun-13:33:57, save_frames=False, save_json=False, save_period=3, save_txt=False, scale=0.5, seed=0, shear=10, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
                        "Overriding model.yaml nc=80 with nc=10\n",
                        "\n",
                        "                   from  n    params  module                                       arguments                     \n",
                        "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
                        "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
                        "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
                        "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
                        "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
                        "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
                        "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
                        "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
                        "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
                        "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
                        " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
                        " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
                        " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                        " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
                        " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
                        " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                        " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
                        " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
                        " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                        " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
                        " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
                        " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                        " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
                        " 23        [16, 19, 22]  1    432622  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
                        "YOLO11n summary: 181 layers, 2,591,790 parameters, 2,591,774 gradients, 6.5 GFLOPs\n",
                        "\n",
                        "Transferred 448/499 items from pretrained weights\n",
                        "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train/artificial_created_mult_back_rotated_big_yolol_advanced_pipe18Jun-13:33:57', view at http://localhost:6006/\n",
                        "Freezing layer 'model.23.dfl.conv.weight'\n",
                        "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
                        "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2962.0¬±599.0 MB/s, size: 469.5 KB)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /data22/stu236894/GitRepos/TinyML-MT/huggingface/ai_shelf/artificial_rotated_mult_back_advanced_pipe/labels/train... 3000 images, 0 backgrounds, 9 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:01<00:00, 1632.47it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mtrain: \u001b[0m/data22/stu236894/GitRepos/TinyML-MT/huggingface/ai_shelf/artificial_rotated_mult_back_advanced_pipe/images/train/generated_image_1007.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.006       1.004]\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0m/data22/stu236894/GitRepos/TinyML-MT/huggingface/ai_shelf/artificial_rotated_mult_back_advanced_pipe/images/train/generated_image_1135.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.002]\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0m/data22/stu236894/GitRepos/TinyML-MT/huggingface/ai_shelf/artificial_rotated_mult_back_advanced_pipe/images/train/generated_image_1250.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.008]\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0m/data22/stu236894/GitRepos/TinyML-MT/huggingface/ai_shelf/artificial_rotated_mult_back_advanced_pipe/images/train/generated_image_1437.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.006]\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0m/data22/stu236894/GitRepos/TinyML-MT/huggingface/ai_shelf/artificial_rotated_mult_back_advanced_pipe/images/train/generated_image_1811.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.004]\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0m/data22/stu236894/GitRepos/TinyML-MT/huggingface/ai_shelf/artificial_rotated_mult_back_advanced_pipe/images/train/generated_image_198.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.008]\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0m/data22/stu236894/GitRepos/TinyML-MT/huggingface/ai_shelf/artificial_rotated_mult_back_advanced_pipe/images/train/generated_image_347.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.002]\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0m/data22/stu236894/GitRepos/TinyML-MT/huggingface/ai_shelf/artificial_rotated_mult_back_advanced_pipe/images/train/generated_image_351.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.008]\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0m/data22/stu236894/GitRepos/TinyML-MT/huggingface/ai_shelf/artificial_rotated_mult_back_advanced_pipe/images/train/generated_image_935.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.002       1.002]\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /data22/stu236894/GitRepos/TinyML-MT/huggingface/ai_shelf/artificial_rotated_mult_back_advanced_pipe/labels/train.cache\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2068.8¬±1616.6 MB/s, size: 531.6 KB)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mval: \u001b[0mScanning /data22/stu236894/GitRepos/TinyML-MT/huggingface/ai_shelf/artificial_rotated_mult_back_advanced_pipe/labels/val... 1000 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 1145.13it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /data22/stu236894/GitRepos/TinyML-MT/huggingface/ai_shelf/artificial_rotated_mult_back_advanced_pipe/labels/val.cache\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Plotting labels to runs/train/artificial_created_mult_back_rotated_big_yolol_advanced_pipe18Jun-13:33:57/labels.jpg... \n",
                        "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
                        "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
                        "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n",
                        "Image sizes 512 train, 512 val\n",
                        "Using 8 dataloader workers\n",
                        "Logging results to \u001b[1mruns/train/artificial_created_mult_back_rotated_big_yolol_advanced_pipe18Jun-13:33:57\u001b[0m\n",
                        "Starting training for 30 epochs...\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       1/30      1.75G      1.738      3.305       1.89        176        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:32<00:00,  5.69it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:06<00:00,  5.29it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.475      0.565      0.468      0.156\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       2/30      1.92G      1.452      2.108      1.728        237        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  5.91it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.37it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.756      0.756       0.81       0.36\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       3/30      1.93G      1.365      1.818      1.671        178        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:30<00:00,  6.10it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.50it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.861      0.874      0.922      0.536\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       4/30      1.93G      1.329      1.725       1.65        216        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  5.99it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:06<00:00,  5.21it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.883      0.888       0.93      0.542\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       5/30      1.93G      1.294      1.651      1.621        198        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  5.99it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:06<00:00,  5.18it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.896      0.898      0.943      0.626\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       6/30      1.93G      1.281      1.604      1.611        186        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  5.93it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:06<00:00,  4.86it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.825      0.831      0.868      0.402\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       7/30      1.93G      1.251      1.552      1.586        189        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  5.97it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:06<00:00,  5.09it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.923      0.929      0.957      0.612\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       8/30      1.93G      1.238      1.526       1.57        200        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  5.92it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.52it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.933      0.934       0.96        0.6\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       9/30      1.93G      1.224      1.492      1.556        242        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  5.92it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.47it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.934      0.934      0.961      0.578\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      10/30      1.93G      1.213      1.466      1.551        182        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  5.96it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.45it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.926      0.907      0.963      0.659\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      11/30      1.93G      1.205      1.439      1.544        200        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  5.95it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:06<00:00,  5.31it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.946      0.942      0.967      0.654\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      12/30      1.93G      1.185      1.402      1.526        218        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  6.01it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:06<00:00,  4.97it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.945      0.947      0.971       0.66\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      13/30      1.93G      1.175      1.394      1.528        191        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:30<00:00,  6.06it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.37it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.954       0.95      0.971      0.655\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      14/30      1.93G       1.17      1.365      1.518        148        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:30<00:00,  6.06it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.49it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.952      0.949       0.97      0.625\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      15/30      1.93G      1.173      1.375       1.52        180        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  5.97it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.48it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.962      0.961      0.978      0.665\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      16/30      1.93G      1.152      1.333      1.505        185        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  5.96it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.36it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.963      0.962      0.982      0.688\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      17/30      1.93G      1.157      1.333      1.508        265        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  6.03it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:06<00:00,  5.20it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.961      0.961      0.983      0.688\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      18/30      1.93G      1.147      1.313        1.5        200        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  5.89it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:06<00:00,  5.23it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.969      0.963      0.984      0.704\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      19/30      1.93G      1.133      1.297      1.495        227        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  6.00it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:06<00:00,  5.30it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.961      0.965       0.98      0.705\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      20/30      1.93G      1.141      1.304      1.497        210        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  6.03it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.39it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.966      0.968      0.983      0.703\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Closing dataloader mosaic\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      21/30      1.93G     0.9084     0.8516      1.294         69        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:31<00:00,  6.02it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.40it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.948      0.954      0.975      0.692\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      22/30      1.93G     0.8952     0.8026      1.293         67        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:30<00:00,  6.09it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.58it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.967      0.964      0.978      0.707\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      23/30      1.93G     0.8868     0.7753      1.279        110        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:30<00:00,  6.13it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.38it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.971      0.974      0.986      0.701\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      24/30      1.93G     0.8752      0.746      1.266         79        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:30<00:00,  6.12it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.44it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.974      0.975      0.981      0.728\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      25/30      1.93G     0.8657     0.7352      1.264         79        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:30<00:00,  6.10it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.43it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.977      0.976      0.983      0.723\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      26/30      1.93G     0.8554     0.7214      1.253        104        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:30<00:00,  6.13it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.57it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399       0.98      0.977      0.985      0.735\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      27/30      1.93G     0.8423     0.7078      1.242         85        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:29<00:00,  6.31it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.55it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.974      0.979      0.986      0.737\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      28/30      1.93G     0.8422     0.6906      1.245         74        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:29<00:00,  6.29it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.63it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.979      0.979      0.987       0.74\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      29/30      1.93G     0.8366     0.6899      1.238         92        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:29<00:00,  6.27it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.68it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.979      0.981      0.988      0.747\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      30/30      1.93G     0.8311     0.6815      1.232         51        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:29<00:00,  6.29it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:05<00:00,  5.67it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.981       0.98      0.988      0.748\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "30 epochs completed in 0.314 hours.\n",
                        "Optimizer stripped from runs/train/artificial_created_mult_back_rotated_big_yolol_advanced_pipe18Jun-13:33:57/weights/last.pt, 5.5MB\n",
                        "Optimizer stripped from runs/train/artificial_created_mult_back_rotated_big_yolol_advanced_pipe18Jun-13:33:57/weights/best.pt, 5.5MB\n",
                        "\n",
                        "Validating runs/train/artificial_created_mult_back_rotated_big_yolol_advanced_pipe18Jun-13:33:57/weights/best.pt...\n",
                        "Ultralytics 8.3.129 üöÄ Python-3.10.12 torch-2.5.1+cu124 CUDA:1 (NVIDIA TITAN Xp, 12183MiB)\n",
                        "YOLO11n summary (fused): 100 layers, 2,584,102 parameters, 0 gradients, 6.3 GFLOPs\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.21it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.981       0.98      0.988      0.748\n",
                        "                 apple        431        591      0.984      0.955      0.985      0.732\n",
                        "               avocado        413        561      0.996      0.998      0.992      0.691\n",
                        "                banana        387        534      0.983      0.993      0.993      0.654\n",
                        "                coffee        400        546      0.958      0.996      0.979      0.774\n",
                        "              cucumber        346        442      0.981      0.998      0.992      0.745\n",
                        "             fruit tea        399        552      0.995      0.989      0.995      0.807\n",
                        "                 lemon        413        611      0.968      0.979      0.979      0.686\n",
                        "               oatmeal        345        441       0.98      0.966      0.988      0.785\n",
                        "                 pasta        399        544      0.968      0.932      0.983      0.787\n",
                        "          tomato sauce        410        577      0.995      0.998      0.995       0.82\n",
                        "Speed: 0.1ms preprocess, 1.9ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
                        "Results saved to \u001b[1mruns/train/artificial_created_mult_back_rotated_big_yolol_advanced_pipe18Jun-13:33:57\u001b[0m\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "Ultralytics 8.3.129 üöÄ Python-3.10.12 torch-2.5.1+cu124 CUDA:0 (NVIDIA TITAN Xp, 12183MiB)\n",
                        "YOLO11n summary (fused): 100 layers, 2,584,102 parameters, 0 gradients, 6.3 GFLOPs\n",
                        "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 3235.3¬±499.4 MB/s, size: 421.8 KB)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mval: \u001b[0mScanning /data22/stu236894/GitRepos/TinyML-MT/huggingface/ai_shelf/artificial_rotated_mult_back_advanced_pipe/labels/val.cache... 1000 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<?, ?it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:07<00:00,  7.94it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       1000       5399      0.981       0.98      0.988      0.748\n",
                        "                 apple        431        591      0.984      0.955      0.985      0.731\n",
                        "               avocado        413        561      0.996      0.998      0.992      0.691\n",
                        "                banana        387        534      0.983      0.993      0.993      0.654\n",
                        "                coffee        400        546      0.958      0.996       0.98      0.775\n",
                        "              cucumber        346        442      0.981      0.998      0.992      0.745\n",
                        "             fruit tea        399        552      0.995      0.989      0.995      0.807\n",
                        "                 lemon        413        611      0.968      0.979       0.98      0.687\n",
                        "               oatmeal        345        441       0.98      0.966      0.988      0.785\n",
                        "                 pasta        399        544      0.968      0.932      0.983      0.786\n",
                        "          tomato sauce        410        577      0.995      0.998      0.995      0.819\n",
                        "Speed: 0.2ms preprocess, 2.1ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
                        "Results saved to \u001b[1m/data22/stu236894/GitRepos/TinyML-MT/runs/detect/val66\u001b[0m\n",
                        "sorted_class_ids_and_names  [(0, 'apple'), (1, 'avocado'), (2, 'banana'), (3, 'coffee'), (4, 'cucumber'), (5, 'fruit tea'), (6, 'lemon'), (7, 'oatmeal'), (8, 'pasta'), (9, 'tomato sauce')]\n"
                    ]
                }
            ],
            "source": [
                "from ultralytics import YOLO\n",
                "import wandb\n",
                "from datetime import datetime\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import wandb\n",
                "NAME_RUN = NAME+datetime.now().strftime(\"%d%b-%H:%M:%S\")\n",
                "# WANB Init with custom name \n",
                "wandb.init(\n",
                "    project=\"Yolo-Training\",\n",
                "    entity=\"maats\",\n",
                "    name=NAME_RUN,\n",
                "    config={  # alle Hyperparameter sauber abspeichern\n",
                "        \"epochs\": EPOCHS,\n",
                "        \"imgsz\": IMG_SIZE,\n",
                "        \"model\": \"yolo11l.pt\",\n",
                "        \"dataset\": DATASET_PATH,\n",
                "    },\n",
                "    sync_tensorboard=True,\n",
                ")\n",
                "\n",
                "try:\n",
                "    # YOLO-Model load\n",
                "    model = YOLO(\"yolo11n.pt\")\n",
                "\n",
                "    # Training\n",
                "    results = model.train(\n",
                "        data=DATASET_PATH,\n",
                "        epochs=EPOCHS,\n",
                "        imgsz=IMG_SIZE,\n",
                "        project=\"runs/train\",\n",
                "        name=NAME_RUN,\n",
                "        verbose=True,\n",
                "        val=True,\n",
                "        save=True,\n",
                "        save_period=3,\n",
                "        mode=\"wandb\",\n",
                "        hsv_h=0.1,\n",
                "        degrees=180,\n",
                "        shear=10,\n",
                "        perspective=0.0003,\n",
                "        mixup = 0.3, # das was Jannek meinte \n",
                "        cutmix = 0.3,\n",
                "        copy_paste = 0.1, # weis ja nicht ...\n",
                "    )\n",
                "  \n",
                "    best_model_path = f\"runs/train/{NAME_RUN}/weights/best.pt\"\n",
                "    model = YOLO(best_model_path)\n",
                "\n",
                "    val_results = model.val(data=DATASET_PATH, imgsz=IMG_SIZE)\n",
                "    log_class_metrics_heatmap(val_results)\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred: {e}\")\n",
                "\n",
                "finally:\n",
                "    # wandb.finish()\n",
                "    # print(\"done\")\n",
                "    pass \n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b20db8b0",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "3697ef6b",
            "metadata": {},
            "source": [
                "## Create Grid for mvtec"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "030f9f16",
            "metadata": {},
            "outputs": [],
            "source": [
                "from ultralytics import YOLO\n",
                "import wandb\n",
                "from datetime import datetime\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import wandb\n",
                "\n",
                "MODEL_PATH = \"runs/train/artificial_created_mult_back_rotated_big_yolol18Jun-00:13:53/weights/best.pt\"\n",
                "model = YOLO(MODEL_PATH)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "0f268df5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtorge-schwark\u001b[0m (\u001b[33mmaats\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "Tracking run with wandb version 0.19.4"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Run data is saved locally in <code>/data22/stu236894/GitRepos/TinyML-MT/code_training/classification-code/wandb/run-20250618_102839-h399va08</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Resuming run <strong><a href='https://wandb.ai/maats/Yolo-Training/runs/h399va08' target=\"_blank\">artificial_created_mult_back_rotated_big_yolol18Jun-00:13:53</a></strong> to <a href='https://wandb.ai/maats/Yolo-Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View project at <a href='https://wandb.ai/maats/Yolo-Training' target=\"_blank\">https://wandb.ai/maats/Yolo-Training</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run at <a href='https://wandb.ai/maats/Yolo-Training/runs/h399va08' target=\"_blank\">https://wandb.ai/maats/Yolo-Training/runs/h399va08</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/maats/Yolo-Training/runs/h399va08?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
                        ],
                        "text/plain": [
                            "<wandb.sdk.wandb_run.Run at 0x7f48a0d59cf0>"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import wandb\n",
                "\n",
                "# Deine Run-ID, z.‚ÄØB. \"ls3jwotb\" aus der URL oder dem lokalen Log\n",
                "run_id = \"h399va08\"\n",
                "\n",
                "# Reaktiviere den Run\n",
                "wandb.init(\n",
                "    project=\"Yolo-Training\",\n",
                "    entity=\"maats\",\n",
                "    id=run_id,\n",
                "    resume=\"allow\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "966a9428",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 1 apple, 2 bananas, 2.5ms\n",
                        "1: 384x512 1 coffee, 1 oatmeal, 2.5ms\n",
                        "2: 384x512 4 avocados, 2.5ms\n",
                        "3: 384x512 2 coffees, 1 oatmeal, 2.5ms\n",
                        "4: 384x512 2 coffees, 2.5ms\n",
                        "5: 384x512 1 pasta, 2.5ms\n",
                        "6: 384x512 (no detections), 2.5ms\n",
                        "7: 384x512 1 coffee, 2.5ms\n",
                        "8: 384x512 3 coffees, 2.5ms\n",
                        "9: 384x512 1 coffee, 2.5ms\n",
                        "10: 384x512 1 oatmeal, 2.5ms\n",
                        "11: 384x512 1 coffee, 2 fruit teas, 2.5ms\n",
                        "12: 384x512 2 coffees, 2.5ms\n",
                        "13: 384x512 (no detections), 2.5ms\n",
                        "14: 384x512 2 coffees, 1 pasta, 2.5ms\n",
                        "15: 384x512 1 coffee, 2 fruit teas, 2.5ms\n",
                        "16: 384x512 3 apples, 3 lemons, 2.5ms\n",
                        "17: 384x512 (no detections), 2.5ms\n",
                        "18: 384x512 1 fruit tea, 1 tomato sauce, 2.5ms\n",
                        "19: 384x512 1 coffee, 2.5ms\n",
                        "Speed: 2.7ms preprocess, 2.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 2 coffees, 2 fruit teas, 1.0ms\n",
                        "1: 384x512 1 coffee, 1 fruit tea, 1 oatmeal, 1.0ms\n",
                        "2: 384x512 2 coffees, 1.0ms\n",
                        "3: 384x512 (no detections), 1.0ms\n",
                        "4: 384x512 4 coffees, 1.0ms\n",
                        "5: 384x512 (no detections), 1.0ms\n",
                        "6: 384x512 (no detections), 1.0ms\n",
                        "7: 384x512 2 coffees, 1.0ms\n",
                        "8: 384x512 2 bananas, 1.0ms\n",
                        "9: 384x512 2 cucumbers, 1.0ms\n",
                        "10: 384x512 2 coffees, 1.0ms\n",
                        "11: 384x512 2 coffees, 1.0ms\n",
                        "12: 384x512 3 coffees, 1 fruit tea, 1.0ms\n",
                        "13: 384x512 1 tomato sauce, 1.0ms\n",
                        "14: 384x512 (no detections), 1.0ms\n",
                        "15: 384x512 (no detections), 1.0ms\n",
                        "16: 384x512 2 avocados, 1 lemon, 1.0ms\n",
                        "17: 384x512 2 apples, 2 avocados, 1 lemon, 1.0ms\n",
                        "18: 384x512 1 apple, 1 banana, 1.0ms\n",
                        "19: 384x512 3 avocados, 1.0ms\n",
                        "Speed: 1.7ms preprocess, 1.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 3 coffees, 1.0ms\n",
                        "1: 384x512 1 oatmeal, 1.0ms\n",
                        "2: 384x512 1 oatmeal, 1.0ms\n",
                        "3: 384x512 2 coffees, 1 fruit tea, 1.0ms\n",
                        "4: 384x512 (no detections), 1.0ms\n",
                        "5: 384x512 (no detections), 1.0ms\n",
                        "6: 384x512 2 coffees, 1.0ms\n",
                        "7: 384x512 2 coffees, 2 fruit teas, 1.0ms\n",
                        "8: 384x512 2 coffees, 1.0ms\n",
                        "9: 384x512 3 coffees, 1.0ms\n",
                        "10: 384x512 1 banana, 1.0ms\n",
                        "11: 384x512 2 coffees, 1.0ms\n",
                        "12: 384x512 1 banana, 1.0ms\n",
                        "13: 384x512 1 coffee, 1.0ms\n",
                        "14: 384x512 1 apple, 2 lemons, 1.0ms\n",
                        "15: 384x512 1 coffee, 1.0ms\n",
                        "16: 384x512 (no detections), 1.0ms\n",
                        "17: 384x512 (no detections), 1.0ms\n",
                        "18: 384x512 3 coffees, 1 fruit tea, 1.0ms\n",
                        "19: 384x512 (no detections), 1.0ms\n",
                        "Speed: 1.5ms preprocess, 1.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 1 coffee, 2 fruit teas, 1.0ms\n",
                        "1: 384x512 (no detections), 1.0ms\n",
                        "2: 384x512 2 coffees, 1.0ms\n",
                        "3: 384x512 2 oatmeals, 1.0ms\n",
                        "4: 384x512 1 coffee, 1.0ms\n",
                        "5: 384x512 3 coffees, 1.0ms\n",
                        "6: 384x512 1 lemon, 1.0ms\n",
                        "7: 384x512 4 coffees, 1.0ms\n",
                        "8: 384x512 (no detections), 1.0ms\n",
                        "9: 384x512 2 coffees, 1.0ms\n",
                        "10: 384x512 1 tomato sauce, 1.0ms\n",
                        "11: 384x512 (no detections), 1.0ms\n",
                        "12: 384x512 1 oatmeal, 1.0ms\n",
                        "13: 384x512 1 cucumber, 1.0ms\n",
                        "14: 384x512 2 coffees, 1 fruit tea, 1.0ms\n",
                        "15: 384x512 1 apple, 2 avocados, 1.0ms\n",
                        "16: 384x512 1 lemon, 1.0ms\n",
                        "17: 384x512 3 avocados, 1.0ms\n",
                        "18: 384x512 (no detections), 1.0ms\n",
                        "19: 384x512 4 avocados, 1.0ms\n",
                        "Speed: 1.5ms preprocess, 1.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 2 apples, 1 avocado, 1 lemon, 1.0ms\n",
                        "1: 384x512 (no detections), 1.0ms\n",
                        "2: 384x512 1 apple, 1.0ms\n",
                        "3: 384x512 1 oatmeal, 1.0ms\n",
                        "4: 384x512 1 oatmeal, 1.0ms\n",
                        "5: 384x512 1 coffee, 1.0ms\n",
                        "6: 384x512 1 oatmeal, 1.0ms\n",
                        "7: 384x512 2 apples, 1 lemon, 1.0ms\n",
                        "8: 384x512 1 coffee, 1.0ms\n",
                        "9: 384x512 1 coffee, 3 fruit teas, 1 oatmeal, 1.0ms\n",
                        "10: 384x512 1 coffee, 1 oatmeal, 1.0ms\n",
                        "11: 384x512 3 coffees, 1.0ms\n",
                        "12: 384x512 1 coffee, 2 fruit teas, 1.0ms\n",
                        "13: 384x512 (no detections), 1.0ms\n",
                        "14: 384x512 1 oatmeal, 1.0ms\n",
                        "15: 384x512 1 lemon, 1.0ms\n",
                        "16: 384x512 (no detections), 1.0ms\n",
                        "17: 384x512 1 coffee, 1.0ms\n",
                        "18: 384x512 1 coffee, 1.0ms\n",
                        "19: 384x512 2 coffees, 1.0ms\n",
                        "Speed: 1.5ms preprocess, 1.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 (no detections), 1.1ms\n",
                        "1: 384x512 1 apple, 2 lemons, 1.1ms\n",
                        "2: 384x512 2 coffees, 1.1ms\n",
                        "3: 384x512 1 apple, 1 banana, 1.1ms\n",
                        "4: 384x512 1 coffee, 2 fruit teas, 1.1ms\n",
                        "5: 384x512 (no detections), 1.1ms\n",
                        "6: 384x512 1 apple, 4 bananas, 1.1ms\n",
                        "7: 384x512 (no detections), 1.1ms\n",
                        "8: 384x512 1 oatmeal, 1.1ms\n",
                        "9: 384x512 5 bananas, 1.1ms\n",
                        "10: 384x512 1 coffee, 1.1ms\n",
                        "11: 384x512 3 coffees, 1.1ms\n",
                        "12: 384x512 1 apple, 2 lemons, 1.1ms\n",
                        "13: 384x512 3 apples, 2 lemons, 1.1ms\n",
                        "14: 384x512 2 oatmeals, 1.1ms\n",
                        "15: 384x512 1 oatmeal, 1.1ms\n",
                        "16: 384x512 2 oatmeals, 1.1ms\n",
                        "17: 384x512 2 apples, 1 avocado, 2 lemons, 1.1ms\n",
                        "18: 384x512 (no detections), 1.1ms\n",
                        "19: 384x512 1 tomato sauce, 1.1ms\n",
                        "Speed: 2.2ms preprocess, 1.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 2 bananas, 1.1ms\n",
                        "1: 384x512 3 avocados, 1.1ms\n",
                        "2: 384x512 1 oatmeal, 1.1ms\n",
                        "3: 384x512 (no detections), 1.1ms\n",
                        "4: 384x512 1 cucumber, 1.1ms\n",
                        "5: 384x512 1 coffee, 1.1ms\n",
                        "6: 384x512 2 apples, 2 avocados, 1 lemon, 1.1ms\n",
                        "7: 384x512 2 coffees, 1.1ms\n",
                        "8: 384x512 (no detections), 1.1ms\n",
                        "9: 384x512 3 apples, 3 lemons, 1.1ms\n",
                        "10: 384x512 (no detections), 1.1ms\n",
                        "11: 384x512 (no detections), 1.1ms\n",
                        "12: 384x512 (no detections), 1.1ms\n",
                        "13: 384x512 3 apples, 1.1ms\n",
                        "14: 384x512 1 oatmeal, 1.1ms\n",
                        "15: 384x512 2 coffees, 1.1ms\n",
                        "16: 384x512 (no detections), 1.1ms\n",
                        "17: 384x512 2 apples, 1.1ms\n",
                        "18: 384x512 2 coffees, 1.1ms\n",
                        "19: 384x512 1 banana, 2 coffees, 1 fruit tea, 1.1ms\n",
                        "Speed: 1.9ms preprocess, 1.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 (no detections), 1.1ms\n",
                        "1: 384x512 1 apple, 2 lemons, 1.1ms\n",
                        "2: 384x512 1 coffee, 1 tomato sauce, 1.1ms\n",
                        "3: 384x512 2 coffees, 1.1ms\n",
                        "4: 384x512 4 apples, 1.1ms\n",
                        "5: 384x512 1 banana, 1.1ms\n",
                        "6: 384x512 1 coffee, 1.1ms\n",
                        "7: 384x512 1 tomato sauce, 1.1ms\n",
                        "8: 384x512 2 coffees, 4 fruit teas, 1.1ms\n",
                        "9: 384x512 1 coffee, 1.1ms\n",
                        "10: 384x512 1 banana, 1.1ms\n",
                        "11: 384x512 1 apple, 1 avocado, 1 lemon, 1.1ms\n",
                        "12: 384x512 1 apple, 2 bananas, 1.1ms\n",
                        "13: 384x512 4 coffees, 1.1ms\n",
                        "14: 384x512 (no detections), 1.1ms\n",
                        "15: 384x512 3 coffees, 1 fruit tea, 1.1ms\n",
                        "16: 384x512 (no detections), 1.1ms\n",
                        "17: 384x512 (no detections), 1.1ms\n",
                        "18: 384x512 3 coffees, 1.1ms\n",
                        "19: 384x512 1 coffee, 1.1ms\n",
                        "Speed: 1.9ms preprocess, 1.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 (no detections), 1.1ms\n",
                        "1: 384x512 3 coffees, 1.1ms\n",
                        "2: 384x512 1 lemon, 1.1ms\n",
                        "3: 384x512 1 coffee, 2 fruit teas, 1.1ms\n",
                        "4: 384x512 2 oatmeals, 1.1ms\n",
                        "5: 384x512 2 coffees, 1 fruit tea, 1.1ms\n",
                        "6: 384x512 1 coffee, 1.1ms\n",
                        "7: 384x512 1 coffee, 1.1ms\n",
                        "8: 384x512 (no detections), 1.1ms\n",
                        "9: 384x512 3 coffees, 1 fruit tea, 1.1ms\n",
                        "10: 384x512 2 bananas, 1.1ms\n",
                        "11: 384x512 1 coffee, 1.1ms\n",
                        "12: 384x512 1 coffee, 1.1ms\n",
                        "13: 384x512 2 apples, 1 banana, 1.1ms\n",
                        "14: 384x512 1 coffee, 1.1ms\n",
                        "15: 384x512 4 avocados, 1.1ms\n",
                        "16: 384x512 1 coffee, 3 fruit teas, 1 oatmeal, 1.1ms\n",
                        "17: 384x512 2 coffees, 1.1ms\n",
                        "18: 384x512 1 coffee, 1.1ms\n",
                        "19: 384x512 (no detections), 1.1ms\n",
                        "Speed: 1.7ms preprocess, 1.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 (no detections), 1.1ms\n",
                        "1: 384x512 1 oatmeal, 1.1ms\n",
                        "2: 384x512 1 coffee, 1 oatmeal, 1.1ms\n",
                        "3: 384x512 1 apple, 1 banana, 1.1ms\n",
                        "4: 384x512 2 cucumbers, 1.1ms\n",
                        "5: 384x512 1 pasta, 1.1ms\n",
                        "6: 384x512 (no detections), 1.1ms\n",
                        "7: 384x512 1 coffee, 1.1ms\n",
                        "8: 384x512 2 coffees, 1 fruit tea, 1.1ms\n",
                        "9: 384x512 (no detections), 1.1ms\n",
                        "10: 384x512 3 apples, 2 lemons, 1.1ms\n",
                        "11: 384x512 (no detections), 1.1ms\n",
                        "12: 384x512 3 apples, 2 lemons, 1.1ms\n",
                        "13: 384x512 3 fruit teas, 1.1ms\n",
                        "14: 384x512 1 banana, 1.1ms\n",
                        "15: 384x512 2 coffees, 1.1ms\n",
                        "16: 384x512 (no detections), 1.1ms\n",
                        "17: 384x512 4 apples, 1.1ms\n",
                        "18: 384x512 1 coffee, 1.1ms\n",
                        "19: 384x512 1 pasta, 1.1ms\n",
                        "Speed: 1.9ms preprocess, 1.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 512)\n"
                    ]
                }
            ],
            "source": [
                "import wandb\n",
                "from ultralytics import YOLO\n",
                "import find_usefull_images_scripts as im_script\n",
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "# Modell laden (ggf. Pfad anpassen)\n",
                "#model = YOLO(\"runs/train/YOLO11n-COCO11-first_artificial_created_dataset/weights/best.pt\")\n",
                "\n",
                "# Alle Beispielbilder laden\n",
                "# image_paths, _ = im_script.get_mvtec_images_for_first_artificial_dataset_classes()\n",
                "\n",
                "\n",
                "# 1.\n",
                "# image_paths, _ = im_script.get_mvtec_images_for_first_artificial_dataset_classes_trained_on_10_clases()\n",
                "\n",
                "\n",
                "image_paths, _ = im_script.get_mvtec_images_for_10classes_dataset()\n",
                "\n",
                "\n",
                "batch_size = 20\n",
                "num_grids = 10\n",
                "\n",
                "for grid_idx in range(num_grids):\n",
                "    start_idx = grid_idx * batch_size\n",
                "    end_idx = start_idx + batch_size\n",
                "    selected_paths = image_paths[start_idx:end_idx]\n",
                "\n",
                "    # Vorhersagen durchf√ºhren (Batch)\n",
                "    preds = model.predict(\n",
                "        selected_paths,\n",
                "        imgsz=IMG_SIZE,\n",
                "        save=False,\n",
                "        stream=False\n",
                "    )\n",
                "\n",
                "    # Bilder vorbereiten\n",
                "    images_drawn = []\n",
                "    for img_path, pred in zip(selected_paths, preds):\n",
                "        img = cv2.imread(str(img_path))\n",
                "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "\n",
                "        h, w, _ = img.shape\n",
                "        for box, cls, conf in zip(pred.boxes.xyxy, pred.boxes.cls, pred.boxes.conf):\n",
                "            x1, y1, x2, y2 = map(int, box)\n",
                "            class_name = model.names[int(cls)]\n",
                "            label = f\"{class_name} {conf:.2f}\"\n",
                "\n",
                "            # Rechteck zeichnen\n",
                "            cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
                "\n",
                "            # Textgr√∂√üe bestimmen\n",
                "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
                "            font_scale = 1.2\n",
                "            thickness = 2\n",
                "            (text_w, text_h), baseline = cv2.getTextSize(label, font, font_scale, thickness)\n",
                "\n",
                "            # Textposition\n",
                "            text_x = x1\n",
                "            if y1 - text_h - baseline > 0:\n",
                "                text_y = y1 - 5\n",
                "                # Hintergrundrechteck f√ºr Text (oben)\n",
                "                cv2.rectangle(img, (text_x, text_y - text_h - baseline), (text_x + text_w, text_y + baseline), (0, 255, 0), cv2.FILLED)\n",
                "                cv2.putText(img, label, (text_x, text_y), font, font_scale, (0, 0, 0), thickness)\n",
                "            else:\n",
                "                text_y = y2 + text_h + 5\n",
                "                if text_y > h:\n",
                "                    text_y = y2 - 5\n",
                "                # Hintergrundrechteck f√ºr Text (unten)\n",
                "                cv2.rectangle(img, (text_x, text_y - text_h - baseline), (text_x + text_w, text_y + baseline), (0, 255, 0), cv2.FILLED)\n",
                "                cv2.putText(img, label, (text_x, text_y), font, font_scale, (0, 0, 0), thickness)\n",
                "\n",
                "        images_drawn.append(img)\n",
                "\n",
                "    # 5x4 Grid erstellen\n",
                "    rows, cols = 5, 4\n",
                "    fig, axs = plt.subplots(rows, cols, figsize=(12, 15), dpi=300)\n",
                "    plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, hspace=0.05, wspace=0.05)\n",
                "\n",
                "    for i, ax in enumerate(axs.flat):\n",
                "        if i < len(images_drawn):\n",
                "            ax.imshow(images_drawn[i])\n",
                "            ax.axis('off')\n",
                "        else:\n",
                "            ax.axis('off')\n",
                "\n",
                "    # Grid als Bild speichern\n",
                "    grid_img_path = f\"prediction_grid_{grid_idx+1}.jpg\"\n",
                "    fig.savefig(grid_img_path, bbox_inches='tight', pad_inches=0)\n",
                "    plt.close(fig)\n",
                "\n",
                "    # Bild bei wandb loggen\n",
                "    wandb.log({f\"grids/mvtec/10_classes/prediction_grid{grid_idx+1}\": wandb.Image(grid_img_path)})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cd112dc0",
            "metadata": {},
            "source": [
                "## calculate Metrixs on MVTEC "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "cbbd30d0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "Ultralytics 8.3.129 üöÄ Python-3.10.12 torch-2.5.1+cu124 CUDA:0 (NVIDIA TITAN Xp, 12183MiB)\n",
                        "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1946.2¬±996.5 MB/s, size: 142.6 KB)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mval: \u001b[0mScanning /data22/stu236894/GitRepos/TinyML-MT/huggingface/full_classes_trained_on_10classes/labels/test.cache... 2124 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2124/2124 [00:00<?, ?it/s]\n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 133/133 [00:13<00:00, 10.22it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all       2124       3884      0.375       0.52      0.349      0.131\n",
                        "                 apple        230        656      0.411      0.514      0.434       0.15\n",
                        "               avocado        150        270      0.374      0.784       0.37      0.127\n",
                        "                banana         53         54     0.0822      0.537     0.0982     0.0304\n",
                        "                coffee        604        879      0.286      0.734       0.36      0.165\n",
                        "              cucumber         91         91      0.531       0.56       0.58      0.169\n",
                        "             fruit tea        767       1538      0.507      0.269      0.304      0.133\n",
                        "                 pasta        391        396      0.435      0.245      0.295      0.145\n",
                        "Speed: 0.1ms preprocess, 1.4ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
                        "Results saved to \u001b[1m/data22/stu236894/GitRepos/TinyML-MT/runs/detect/val67\u001b[0m\n",
                        "0.37501995669451427 0.5204666616041824 0.38957328338065833\n",
                        "sorted_class_ids_and_names  [(0, 'apple'), (1, 'avocado'), (2, 'banana'), (3, 'coffee'), (4, 'cucumber'), (5, 'fruit tea'), (6, 'lemon'), (7, 'oatmeal'), (8, 'pasta'), (9, 'tomato sauce')]\n"
                    ]
                }
            ],
            "source": [
                "from ultralytics import YOLO\n",
                "\n",
                "# Modell laden\n",
                "path = \"../../huggingface/full_classes_trained_on_10classes/dataset.yaml\"\n",
                "# null_classes for big \n",
                "null_clases = [\"lemon\", \"oatmeal\", \"tomato sauce\"]\n",
                "# for small dataset \n",
                "# null_clases = [\"coffee\", \"lemon\", \"oatmeal\", \"pasta\", \"tomato sauce\"]\n",
                "\n",
                "\n",
                "absolute_path = os.path.abspath(path)\n",
                "# Evaluation auf dem 'test' Teil des Datasets\n",
                "metrics = model.val(\n",
                "    data=absolute_path,  \n",
                "    split='test',              \n",
                "    imgsz=IMG_SIZE               \n",
                ")\n",
                "\n",
                "print(np.mean(metrics.box.p),np.mean(metrics.box.r), np.mean(metrics.box.f1))\n",
                "log_class_metrics_heatmap(metrics, null_classes= null_clases, wandb_key=\"test_mvtec_10_clases/heatmap\")\n",
                "wandb.log({\n",
                "    \"test_mvtec_10_clases/mAP50_class_normal\": float(metrics.box.map50),\n",
                "    \"test_mvtec_10_clases/precision_class_normal\": float(np.mean(metrics.box.p)),\n",
                "    \"test_mvtec_10_clases/recall_class_normal\": float(np.mean(metrics.box.r)),\n",
                "    \"test_mvtec_10_clases/f1_class_normal\": float(np.mean(metrics.box.f1)),\n",
                "    \"test_mvtec_10_clases/mAP50-95_class_normal\": float(metrics.box.map),\n",
                "})\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cc1b1ecd",
            "metadata": {},
            "source": [
                "## calculate grid for custom"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "b3763fe9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 2 coffees, 1.1ms\n",
                        "1: 384x512 1 lemon, 1 tomato sauce, 1.1ms\n",
                        "2: 384x512 1 oatmeal, 1 pasta, 1.1ms\n",
                        "3: 384x512 2 coffees, 1.1ms\n",
                        "4: 384x512 1 apple, 1.1ms\n",
                        "5: 384x512 2 coffees, 1.1ms\n",
                        "6: 384x512 2 bananas, 1 coffee, 1 pasta, 1.1ms\n",
                        "7: 384x512 (no detections), 1.1ms\n",
                        "8: 384x512 1 avocado, 1 lemon, 1.1ms\n",
                        "9: 384x512 1 apple, 1 avocado, 1.1ms\n",
                        "10: 384x512 2 apples, 1.1ms\n",
                        "11: 384x512 1 avocado, 1 tomato sauce, 1.1ms\n",
                        "12: 384x512 1 coffee, 2 lemons, 1.1ms\n",
                        "13: 384x512 1 coffee, 1.1ms\n",
                        "14: 384x512 1 banana, 1.1ms\n",
                        "15: 384x512 1 coffee, 2 tomato sauces, 1.1ms\n",
                        "16: 384x512 1 coffee, 1 lemon, 1.1ms\n",
                        "17: 384x512 1 avocado, 1 coffee, 1 oatmeal, 1.1ms\n",
                        "18: 384x512 1 apple, 1 coffee, 1.1ms\n",
                        "19: 384x512 1 apple, 1 banana, 1 tomato sauce, 1.1ms\n",
                        "Speed: 3.4ms preprocess, 1.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 1 banana, 1 lemon, 1 pasta, 1.1ms\n",
                        "1: 384x512 1 avocado, 1 lemon, 1.1ms\n",
                        "2: 384x512 1 apple, 1.1ms\n",
                        "3: 384x512 1 apple, 1.1ms\n",
                        "4: 384x512 2 apples, 2 avocados, 1 fruit tea, 3 lemons, 1 oatmeal, 1.1ms\n",
                        "5: 384x512 1 lemon, 1.1ms\n",
                        "6: 384x512 2 apples, 1.1ms\n",
                        "7: 384x512 (no detections), 1.1ms\n",
                        "8: 384x512 2 avocados, 2 bananas, 2 coffees, 1 lemon, 1.1ms\n",
                        "9: 384x512 2 fruit teas, 1 oatmeal, 1.1ms\n",
                        "10: 384x512 1 coffee, 2 lemons, 1.1ms\n",
                        "11: 384x512 1 apple, 2 avocados, 1.1ms\n",
                        "12: 384x512 3 apples, 2 avocados, 1 fruit tea, 4 lemons, 1.1ms\n",
                        "13: 384x512 1 banana, 1.1ms\n",
                        "14: 384x512 2 apples, 2 bananas, 1 fruit tea, 1 lemon, 2 oatmeals, 1 pasta, 1.1ms\n",
                        "15: 384x512 1 avocado, 1 coffee, 1.1ms\n",
                        "16: 384x512 1 coffee, 1 tomato sauce, 1.1ms\n",
                        "17: 384x512 2 lemons, 1.1ms\n",
                        "18: 384x512 (no detections), 1.1ms\n",
                        "19: 384x512 1 pasta, 1.1ms\n",
                        "Speed: 2.0ms preprocess, 1.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 1 coffee, 1.2ms\n",
                        "1: 384x512 2 coffees, 1 tomato sauce, 1.2ms\n",
                        "2: 384x512 1 avocado, 3 coffees, 1.2ms\n",
                        "3: 384x512 2 tomato sauces, 1.2ms\n",
                        "4: 384x512 1 avocado, 1.2ms\n",
                        "5: 384x512 1 apple, 1 lemon, 1.2ms\n",
                        "6: 384x512 2 avocados, 4 lemons, 1.2ms\n",
                        "7: 384x512 1 pasta, 1.2ms\n",
                        "8: 384x512 1 banana, 1 lemon, 1.2ms\n",
                        "9: 384x512 1 avocado, 1 coffee, 1.2ms\n",
                        "10: 384x512 2 apples, 3 avocados, 3 lemons, 1.2ms\n",
                        "11: 384x512 1 apple, 2 avocados, 2 lemons, 1 pasta, 1.2ms\n",
                        "12: 384x512 3 coffees, 1.2ms\n",
                        "13: 384x512 1 pasta, 1.2ms\n",
                        "14: 384x512 1 fruit tea, 1.2ms\n",
                        "15: 384x512 1 pasta, 1.2ms\n",
                        "16: 384x512 1 coffee, 3 lemons, 1.2ms\n",
                        "17: 384x512 2 apples, 3 avocados, 5 lemons, 1 oatmeal, 1.2ms\n",
                        "18: 384x512 1 coffee, 2 tomato sauces, 1.2ms\n",
                        "19: 384x512 1 avocado, 1 tomato sauce, 1.2ms\n",
                        "Speed: 2.4ms preprocess, 1.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 1 banana, 1 tomato sauce, 1.1ms\n",
                        "1: 384x512 1 apple, 1 coffee, 1 tomato sauce, 1.1ms\n",
                        "2: 384x512 1 coffee, 1 fruit tea, 1 oatmeal, 1.1ms\n",
                        "3: 384x512 2 avocados, 2 coffees, 1 lemon, 1.1ms\n",
                        "4: 384x512 2 lemons, 1.1ms\n",
                        "5: 384x512 1 apple, 2 bananas, 1 coffee, 1.1ms\n",
                        "6: 384x512 1 coffee, 1 lemon, 1.1ms\n",
                        "7: 384x512 2 coffees, 1 cucumber, 1 tomato sauce, 1.1ms\n",
                        "8: 384x512 1 apple, 1 avocado, 1 lemon, 1 pasta, 1.1ms\n",
                        "9: 384x512 1 avocado, 1.1ms\n",
                        "10: 384x512 1 apple, 1 banana, 1 tomato sauce, 1.1ms\n",
                        "11: 384x512 1 tomato sauce, 1.1ms\n",
                        "12: 384x512 1 fruit tea, 1.1ms\n",
                        "13: 384x512 1 coffee, 1 lemon, 1.1ms\n",
                        "14: 384x512 1 avocado, 1 tomato sauce, 1.1ms\n",
                        "15: 384x512 2 bananas, 2 coffees, 1.1ms\n",
                        "16: 384x512 (no detections), 1.1ms\n",
                        "17: 384x512 1 apple, 1 avocado, 1 banana, 1 fruit tea, 1 lemon, 1.1ms\n",
                        "18: 384x512 2 apples, 1 banana, 1 tomato sauce, 1.1ms\n",
                        "19: 384x512 1 avocado, 1.1ms\n",
                        "Speed: 1.6ms preprocess, 1.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping cucumber tensor(4., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 1 apple, 2 cucumbers, 1 fruit tea, 3 lemons, 1 oatmeal, 1 tomato sauce, 1.1ms\n",
                        "1: 384x512 1 avocado, 1 tomato sauce, 1.1ms\n",
                        "2: 384x512 1 apple, 2 avocados, 1 lemon, 1 pasta, 1.1ms\n",
                        "3: 384x512 1 banana, 1 tomato sauce, 1.1ms\n",
                        "4: 384x512 1 coffee, 1 oatmeal, 1.1ms\n",
                        "5: 384x512 1 coffee, 1 oatmeal, 1 pasta, 1.1ms\n",
                        "6: 384x512 1 lemon, 2 oatmeals, 1 tomato sauce, 1.1ms\n",
                        "7: 384x512 1 pasta, 1.1ms\n",
                        "8: 384x512 1 tomato sauce, 1.1ms\n",
                        "9: 384x512 1 apple, 2 avocados, 1 coffee, 1.1ms\n",
                        "10: 384x512 2 apples, 1 lemon, 1.1ms\n",
                        "11: 384x512 1 banana, 1.1ms\n",
                        "12: 384x512 1 coffee, 1.1ms\n",
                        "13: 384x512 1 pasta, 1.1ms\n",
                        "14: 384x512 1 fruit tea, 1.1ms\n",
                        "15: 384x512 1 coffee, 1.1ms\n",
                        "16: 384x512 1 pasta, 1.1ms\n",
                        "17: 384x512 1 pasta, 1.1ms\n",
                        "18: 384x512 2 cucumbers, 1 lemon, 1 pasta, 1.1ms\n",
                        "19: 384x512 1 avocado, 3 coffees, 1 lemon, 1 pasta, 2 tomato sauces, 1.1ms\n",
                        "Speed: 1.6ms preprocess, 1.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping cucumber tensor(4., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping cucumber tensor(4., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping cucumber tensor(4., device='cuda:0')\n",
                        "mapping cucumber tensor(4., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 2 avocados, 2 lemons, 1.1ms\n",
                        "1: 384x512 1 apple, 2 avocados, 1 banana, 2 coffees, 1 lemon, 1 oatmeal, 1.1ms\n",
                        "2: 384x512 4 coffees, 1 lemon, 1.1ms\n",
                        "3: 384x512 1 avocado, 1 tomato sauce, 1.1ms\n",
                        "4: 384x512 2 lemons, 1.1ms\n",
                        "5: 384x512 2 apples, 2 avocados, 2 lemons, 1.1ms\n",
                        "6: 384x512 1 banana, 1 cucumber, 2 fruit teas, 2 lemons, 2 oatmeals, 1.1ms\n",
                        "7: 384x512 2 lemons, 4 oatmeals, 1.1ms\n",
                        "8: 384x512 1 banana, 2 lemons, 1.1ms\n",
                        "9: 384x512 1 coffee, 1 tomato sauce, 1.1ms\n",
                        "10: 384x512 1 fruit tea, 1.1ms\n",
                        "11: 384x512 2 fruit teas, 1.1ms\n",
                        "12: 384x512 1 banana, 1.1ms\n",
                        "13: 384x512 1 tomato sauce, 1.1ms\n",
                        "14: 384x512 1 avocado, 1 coffee, 1 lemon, 1 pasta, 1.1ms\n",
                        "15: 384x512 1 avocado, 3 coffees, 1 lemon, 1 pasta, 1.1ms\n",
                        "16: 384x512 1 apple, 1 coffee, 1.1ms\n",
                        "17: 384x512 3 coffees, 1.1ms\n",
                        "18: 384x512 2 apples, 1 cucumber, 1 fruit tea, 3 lemons, 1.1ms\n",
                        "19: 384x512 1 banana, 1.1ms\n",
                        "Speed: 1.5ms preprocess, 1.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping cucumber tensor(4., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping cucumber tensor(4., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 1 banana, 1 fruit tea, 1 oatmeal, 1.1ms\n",
                        "1: 384x512 4 avocados, 2 lemons, 1.1ms\n",
                        "2: 384x512 1 apple, 1 lemon, 1.1ms\n",
                        "3: 384x512 1 coffee, 2 lemons, 1.1ms\n",
                        "4: 384x512 1 apple, 2 bananas, 1 lemon, 1.1ms\n",
                        "5: 384x512 3 lemons, 1.1ms\n",
                        "6: 384x512 3 coffees, 1.1ms\n",
                        "7: 384x512 1 avocado, 1 lemon, 1.1ms\n",
                        "8: 384x512 1 banana, 1.1ms\n",
                        "9: 384x512 4 apples, 1 banana, 1.1ms\n",
                        "10: 384x512 1 coffee, 1 pasta, 1.1ms\n",
                        "11: 384x512 2 avocados, 1.1ms\n",
                        "12: 384x512 1 banana, 1.1ms\n",
                        "13: 384x512 2 avocados, 1.1ms\n",
                        "14: 384x512 1 pasta, 1 tomato sauce, 1.1ms\n",
                        "15: 384x512 2 oatmeals, 1.1ms\n",
                        "16: 384x512 1 pasta, 1.1ms\n",
                        "17: 384x512 1 coffee, 1 pasta, 1.1ms\n",
                        "18: 384x512 1 avocado, 1 coffee, 2 lemons, 1.1ms\n",
                        "19: 384x512 1 avocado, 1.1ms\n",
                        "Speed: 1.4ms preprocess, 1.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 1 coffee, 1.1ms\n",
                        "1: 384x512 1 coffee, 1.1ms\n",
                        "2: 384x512 1 apple, 2 avocados, 1.1ms\n",
                        "3: 384x512 1 avocado, 1 lemon, 1.1ms\n",
                        "4: 384x512 1 apple, 1 coffee, 1 tomato sauce, 1.1ms\n",
                        "5: 384x512 1 pasta, 1.1ms\n",
                        "6: 384x512 2 apples, 3 bananas, 1.1ms\n",
                        "7: 384x512 2 coffees, 1 lemon, 1 oatmeal, 1.1ms\n",
                        "8: 384x512 1 banana, 1 coffee, 1.1ms\n",
                        "9: 384x512 1 banana, 1.1ms\n",
                        "10: 384x512 1 coffee, 1 lemon, 1.1ms\n",
                        "11: 384x512 2 fruit teas, 1 oatmeal, 1.1ms\n",
                        "12: 384x512 2 avocados, 2 lemons, 1.1ms\n",
                        "13: 384x512 1 banana, 1.1ms\n",
                        "14: 384x512 1 banana, 1.1ms\n",
                        "15: 384x512 2 lemons, 2 oatmeals, 1.1ms\n",
                        "16: 384x512 1 avocado, 1 banana, 1.1ms\n",
                        "17: 384x512 1 apple, 1.1ms\n",
                        "18: 384x512 1 avocado, 2 lemons, 1.1ms\n",
                        "19: 384x512 2 tomato sauces, 1.1ms\n",
                        "Speed: 1.4ms preprocess, 1.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 2 cucumbers, 2 tomato sauces, 1.1ms\n",
                        "1: 384x512 2 coffees, 1 lemon, 1.1ms\n",
                        "2: 384x512 2 avocados, 1 tomato sauce, 1.1ms\n",
                        "3: 384x512 1 banana, 2 coffees, 1.1ms\n",
                        "4: 384x512 2 avocados, 2 lemons, 1.1ms\n",
                        "5: 384x512 3 lemons, 1.1ms\n",
                        "6: 384x512 1 banana, 1 tomato sauce, 1.1ms\n",
                        "7: 384x512 1 avocado, 1 coffee, 1 oatmeal, 1 tomato sauce, 1.1ms\n",
                        "8: 384x512 2 apples, 1 lemon, 1 oatmeal, 2 pastas, 1.1ms\n",
                        "9: 384x512 1 apple, 1 coffee, 2 fruit teas, 1.1ms\n",
                        "10: 384x512 1 coffee, 2 cucumbers, 2 lemons, 1 tomato sauce, 1.1ms\n",
                        "11: 384x512 2 avocados, 3 coffees, 1 pasta, 1.1ms\n",
                        "12: 384x512 1 avocado, 1.1ms\n",
                        "13: 384x512 2 avocados, 1 tomato sauce, 1.1ms\n",
                        "14: 384x512 1 apple, 1 fruit tea, 2 oatmeals, 1.1ms\n",
                        "15: 384x512 1 apple, 1 banana, 1 lemon, 1.1ms\n",
                        "16: 384x512 1 apple, 1 coffee, 1 tomato sauce, 1.1ms\n",
                        "17: 384x512 2 apples, 1 avocado, 1 lemon, 1 pasta, 1.1ms\n",
                        "18: 384x512 1 avocado, 4 bananas, 3 coffees, 1 lemon, 1.1ms\n",
                        "19: 384x512 1 banana, 1 lemon, 1.1ms\n",
                        "Speed: 1.6ms preprocess, 1.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "mapping cucumber tensor(4., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping cucumber tensor(4., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping cucumber tensor(4., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping cucumber tensor(4., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "\n",
                        "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n",
                        "0: 384x512 1 apple, 1 tomato sauce, 1.1ms\n",
                        "1: 384x512 1 banana, 2 lemons, 1.1ms\n",
                        "2: 384x512 (no detections), 1.1ms\n",
                        "3: 384x512 1 apple, 3 avocados, 1 lemon, 1 oatmeal, 1.1ms\n",
                        "4: 384x512 1 coffee, 1 lemon, 1.1ms\n",
                        "5: 384x512 1 apple, 2 tomato sauces, 1.1ms\n",
                        "6: 384x512 1 apple, 1 avocado, 2 cucumbers, 1 fruit tea, 1.1ms\n",
                        "7: 384x512 1 apple, 1 avocado, 1.1ms\n",
                        "8: 384x512 1 oatmeal, 1 pasta, 1.1ms\n",
                        "9: 384x512 1 tomato sauce, 1.1ms\n",
                        "10: 384x512 1 banana, 1.1ms\n",
                        "11: 384x512 1 banana, 1 lemon, 3 pastas, 1 tomato sauce, 1.1ms\n",
                        "12: 384x512 1 apple, 5 coffees, 2 cucumbers, 2 fruit teas, 3 lemons, 1.1ms\n",
                        "13: 384x512 2 lemons, 1.1ms\n",
                        "14: 384x512 1 coffee, 1 tomato sauce, 1.1ms\n",
                        "15: 384x512 1 apple, 1 tomato sauce, 1.1ms\n",
                        "16: 384x512 1 oatmeal, 1.1ms\n",
                        "17: 384x512 1 apple, 1.1ms\n",
                        "18: 384x512 1 coffee, 2 lemons, 1.1ms\n",
                        "19: 384x512 1 avocado, 2 coffees, 1 lemon, 1 tomato sauce, 1.1ms\n",
                        "Speed: 1.5ms preprocess, 1.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping cucumber tensor(4., device='cuda:0')\n",
                        "mapping cucumber tensor(4., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping banana tensor(2., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping pasta tensor(8., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping cucumber tensor(4., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping cucumber tensor(4., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping fruit tea tensor(5., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping oatmeal tensor(7., device='cuda:0')\n",
                        "mapping apple tensor(0., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping avocado tensor(1., device='cuda:0')\n",
                        "mapping lemon tensor(6., device='cuda:0')\n",
                        "mapping coffee tensor(3., device='cuda:0')\n",
                        "mapping tomato sauce tensor(9., device='cuda:0')\n"
                    ]
                }
            ],
            "source": [
                "import wandb\n",
                "from ultralytics import YOLO\n",
                "import find_usefull_images_scripts as im_script\n",
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "# Alle Beispielbilder laden\n",
                "image_paths, _ = im_script.get_custom_10class_class_dataset()\n",
                "\n",
                "#image_paths, _ = im_script.get_custom_small_class_dataset()\n",
                "\n",
                "batch_size = 20\n",
                "num_grids = 10\n",
                "\n",
                "for grid_idx in range(num_grids):\n",
                "    start_idx = grid_idx * batch_size\n",
                "    end_idx = start_idx + batch_size\n",
                "    selected_paths = image_paths[start_idx:end_idx]\n",
                "\n",
                "    # Vorhersagen durchf√ºhren (Batch)\n",
                "    preds = model.predict(\n",
                "        selected_paths,\n",
                "        imgsz=IMG_SIZE,\n",
                "        save=False,\n",
                "        stream=False\n",
                "    )\n",
                "\n",
                "    # Bilder vorbereiten\n",
                "    images_drawn = []\n",
                "    for img_path, pred in zip(selected_paths, preds):\n",
                "        img = cv2.imread(str(img_path))\n",
                "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "\n",
                "        h, w, _ = img.shape\n",
                "        for box, cls, conf in zip(pred.boxes.xyxy, pred.boxes.cls, pred.boxes.conf):\n",
                "            x1, y1, x2, y2 = map(int, box)\n",
                "\n",
                "            class_name = model.names[int(cls)]\n",
                "            print(\"mapping\" , class_name, cls)\n",
                "            label = f\"{class_name} {conf:.2f}\"\n",
                "\n",
                "            # Rechteck zeichnen\n",
                "            cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
                "\n",
                "            # Textgr√∂√üe bestimmen\n",
                "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
                "            font_scale = 1.2\n",
                "            thickness = 2\n",
                "            (text_w, text_h), baseline = cv2.getTextSize(label, font, font_scale, thickness)\n",
                "\n",
                "            # Textposition\n",
                "            text_x = x1\n",
                "            if y1 - text_h - baseline > 0:\n",
                "                text_y = y1 - 5\n",
                "                # Hintergrundrechteck f√ºr Text (oben)\n",
                "                cv2.rectangle(img, (text_x, text_y - text_h - baseline), (text_x + text_w, text_y + baseline), (0, 255, 0), cv2.FILLED)\n",
                "                cv2.putText(img, label, (text_x, text_y), font, font_scale, (0, 0, 0), thickness)\n",
                "            else:\n",
                "                text_y = y2 + text_h + 5\n",
                "                if text_y > h:\n",
                "                    text_y = y2 - 5\n",
                "                # Hintergrundrechteck f√ºr Text (unten)\n",
                "                cv2.rectangle(img, (text_x, text_y - text_h - baseline), (text_x + text_w, text_y + baseline), (0, 255, 0), cv2.FILLED)\n",
                "                cv2.putText(img, label, (text_x, text_y), font, font_scale, (0, 0, 0), thickness)\n",
                "\n",
                "        images_drawn.append(img)\n",
                "\n",
                "    # 5x4 Grid erstellen\n",
                "    rows, cols = 5, 4\n",
                "    fig, axs = plt.subplots(rows, cols, figsize=(12, 15), dpi=300)\n",
                "    plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, hspace=0.05, wspace=0.05)\n",
                "\n",
                "    for i, ax in enumerate(axs.flat):\n",
                "        if i < len(images_drawn):\n",
                "            ax.imshow(images_drawn[i])\n",
                "            ax.axis('off')\n",
                "        else:\n",
                "            ax.axis('off')\n",
                "\n",
                "    # Grid als Bild speichern\n",
                "    grid_img_path = f\"prediction_grid_{grid_idx+1}.jpg\"\n",
                "    fig.savefig(grid_img_path, bbox_inches='tight', pad_inches=0)\n",
                "    plt.close(fig)\n",
                "\n",
                "    # Bild bei wandb loggen\n",
                "    wandb.log({f\"/grids/custom/prediction_grid_{grid_idx+1}\": wandb.Image(grid_img_path)})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "71a07f9b",
            "metadata": {},
            "source": [
                "## calculate Metrics for Custom"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e5c85f7d",
            "metadata": {},
            "outputs": [
                {
                    "ename": "ValueError",
                    "evalue": "Sample larger than population or is negative",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[12], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m all_image_paths \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 200 zuf√§llige ausw√§hlen\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m selected_images \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_image_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Grid-Parameter\u001b[39;00m\n\u001b[1;32m     34\u001b[0m grid_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
                        "File \u001b[0;32m/usr/lib/python3.10/random.py:482\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    480\u001b[0m randbelow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n\u001b[0;32m--> 482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample larger than population or is negative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    483\u001b[0m result \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m k\n\u001b[1;32m    484\u001b[0m setsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m        \u001b[38;5;66;03m# size of a small set minus size of an empty list\u001b[39;00m\n",
                        "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import wandb\n",
                "import numpy as np\n",
                "import find_usefull_images_scripts as im_script\n",
                "\n",
                "# # Mapping von Model-Output-Klasse ‚Üí GT-Klasse\n",
                "label_translation_trained_on_10classes = {\n",
                "    0: 1, 1: 3, 2: 4, 3: 13, 4: 48, 5: 26, 6: 2, 7:42, 8: 9, 9: 5\n",
                "}\n",
                "\n",
                "\n",
                "# label_translation_trained_on_small_set = {\n",
                "#     0: 1, 1: 3, 2: 4, 3: 48, 4: 26, 5: 2, 6: 5\n",
                "# }\n",
                "\n",
                "def compute_classnorm_metrics(gt_dicts, pred_dicts):\n",
                "    \"\"\"\n",
                "    Berechnet class-normalisierte Precision, Recall, F1,\n",
                "    wobei Klassen ohne Vorkommen ignoriert werden.\n",
                "    \"\"\"\n",
                "    all_classes = sorted(set().union(*[d.keys() for d in gt_dicts + pred_dicts]))\n",
                "    \n",
                "    classwise_precisions = []\n",
                "    classwise_recalls = []\n",
                "    classwise_f1s = []\n",
                "    classwise_gt_count = []\n",
                "    classwise_pred_count = []\n",
                "    classwise_fp = []\n",
                "    classwise_fn = []\n",
                "    classwise_tp = []\n",
                "\n",
                "    for cls in all_classes:\n",
                "        tp, fp, fn = 0, 0, 0\n",
                "        gt_count_class = 0\n",
                "        pred_count_class = 0\n",
                "        for gt, pred in zip(gt_dicts, pred_dicts):\n",
                "            gt_count = gt.get(cls, 0)\n",
                "            pred_count = pred.get(cls, 0)\n",
                "            gt_count_class += gt_count\n",
                "            pred_count_class += pred_count\n",
                "\n",
                "            tp += min(gt_count, pred_count)\n",
                "            fp += max(0, pred_count - gt_count)\n",
                "            fn += max(0, gt_count - pred_count)\n",
                "\n",
                "\n",
                "        # Skip class if both gt and pred are zero\n",
                "        if cls == 3:\n",
                "            print(\"avocado\", tp, fp, fn)\n",
                "        if (tp + fp + fn) == 0:\n",
                "            continue\n",
                "\n",
                "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
                "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
                "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
                "\n",
                "        classwise_gt_count.append(gt_count_class)\n",
                "        classwise_pred_count.append(pred_count_class)\n",
                "        classwise_fp.append(fp)\n",
                "        classwise_fn.append(fn)\n",
                "        classwise_tp.append(tp)\n",
                "        classwise_precisions.append(precision)\n",
                "        classwise_recalls.append(recall)\n",
                "        classwise_f1s.append(f1)\n",
                "    \n",
                "    mean_precision = np.mean(classwise_precisions) if len(classwise_precisions) > 0 else 0.0\n",
                "    mean_recall = np.mean(classwise_recalls) if len(classwise_recalls) > 0 else 0.0\n",
                "    mean_f1 = np.mean(classwise_f1s) if len(classwise_f1s) > 0 else 0.0\n",
                "\n",
                "    results = {\"class_norm_precision\": mean_precision, \"class_norm_recall\": mean_recall, \"class_norm_f1\": mean_f1, \n",
                "               \"classwise_gt_count\": classwise_gt_count, \"classwise_pred_count\": classwise_pred_count, \"classwise_fp\": classwise_fp, \"classwise_fn\": classwise_fn,\n",
                "               \"classwise_tp\": classwise_tp, \"classwise_precisions\": classwise_precisions, \"classwise_recalls\": classwise_recalls, \"classwise_f1s\" :classwise_f1s }\n",
                "\n",
                "    return results\n",
                "\n",
                "\n",
                "\n",
                "def compute_global_metrics(gt_dicts, pred_dicts):\n",
                "    all_classes = sorted(set().union(*[d.keys() for d in gt_dicts + pred_dicts]))\n",
                "\n",
                "    def dict_to_vec(d, classes):\n",
                "        return np.array([d.get(c, 0) for c in classes], dtype=np.float32)\n",
                "\n",
                "    gt_arr = np.stack([dict_to_vec(d, all_classes) for d in gt_dicts])\n",
                "    pred_arr = np.stack([dict_to_vec(d, all_classes) for d in pred_dicts])\n",
                "    print(gt_arr, pred_arr)\n",
                "\n",
                "    tp = np.minimum(gt_arr, pred_arr).sum()\n",
                "    fp = np.maximum(pred_arr - gt_arr, 0).sum()\n",
                "    fn = np.maximum(gt_arr - pred_arr, 0).sum()\n",
                "\n",
                "\n",
                "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
                "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
                "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
                "\n",
                "    return precision, recall, f1\n",
                "\n",
                "\n",
                "def translate_prediction_counts(pred_classes, translation_dict):\n",
                "    \"\"\"\n",
                "    Z√§hlt vorhergesagte Klassen und √ºbersetzt sie in Zielklassen.\n",
                "    Gibt dict {gt_class_id: count} zur√ºck.\n",
                "    \"\"\"\n",
                "    pred_counts = {}\n",
                "    for c in pred_classes:\n",
                "        mapped = translation_dict[c]\n",
                "        if mapped is not None:\n",
                "            # get is a cool trick standard value of 0 allows to access even though its not initialised\n",
                "            pred_counts[mapped] = pred_counts.get(mapped, 0) + 1\n",
                "    return pred_counts\n",
                "\n",
                "# === Main ===\n",
                "\n",
                "image_paths, label_lines = im_script.get_custom_10class_class_dataset()\n",
                "batch_size = 20\n",
                "\n",
                "all_gt_counts = []\n",
                "all_pred_counts = []\n",
                "\n",
                "for i in range(0, len(image_paths), batch_size):\n",
                "    batch_paths = image_paths[i:i + batch_size]\n",
                "    batch_labels = label_lines[i:i + batch_size]  # dicts: class_id -> count (GT-Klassen)\n",
                "\n",
                "    # GT-Labels direkt √ºbernehmen\n",
                "    all_gt_counts.extend(batch_labels)\n",
                "\n",
                "    # Model Predictions holen\n",
                "    preds_raw = model.predict(batch_paths, imgsz=IMG_SIZE, stream=False)\n",
                "\n",
                "    for i, pred in enumerate(preds_raw):\n",
                "        pred_classes = pred.boxes.cls.cpu().tolist()\n",
                "        translated_pred = translate_prediction_counts(pred_classes, label_translation_trained_on_10classes)\n",
                "        all_pred_counts.append(translated_pred)\n",
                "\n",
                "\n",
                "precision, recall, f1 = compute_global_metrics(all_gt_counts, all_pred_counts)\n",
                "\n",
                "\n",
                "# Zus√§tzlich: class-normalisierte Metriken berechnen\n",
                "results = compute_classnorm_metrics(all_gt_counts, all_pred_counts)\n",
                "\n",
                "\n",
                "\n",
                "print(precision, recall, f1, results[\"class_norm_precision\"], results[\"class_norm_recall\"], results[\"class_norm_f1\"])\n",
                "wandb.log({\n",
                "    \"test/custom/precision_counts\": float(precision),\n",
                "    \"test/custom/recall_counts\": float(recall),\n",
                "    \"test/custom/f1_score_counts\": float(f1),\n",
                "    \"test/custom/precision_classnorm_CARE\": float(results[\"class_norm_precision\"]),\n",
                "    \"test/custom/recall_classnorm_CARE\": float( results[\"class_norm_recall\"]),\n",
                "    \"test/custom/f1_classnorm_CARE\": float( results[\"class_norm_f1\"])\n",
                "}, step=wandb.run.step)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "id": "a300df26",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "Tracking run with wandb version 0.19.4"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Run data is saved locally in <code>/data22/stu236894/GitRepos/TinyML-MT/code_training/classification-code/wandb/run-20250616_224553-havg57rg</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Resuming run <strong><a href='https://wandb.ai/maats/Yolo-Training/runs/havg57rg' target=\"_blank\">fixed_metrics_compare_first_artificial_dataset_again16Jun-16:13:20</a></strong> to <a href='https://wandb.ai/maats/Yolo-Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View project at <a href='https://wandb.ai/maats/Yolo-Training' target=\"_blank\">https://wandb.ai/maats/Yolo-Training</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run at <a href='https://wandb.ai/maats/Yolo-Training/runs/havg57rg' target=\"_blank\">https://wandb.ai/maats/Yolo-Training/runs/havg57rg</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/maats/Yolo-Training/runs/havg57rg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
                        ],
                        "text/plain": [
                            "<wandb.sdk.wandb_run.Run at 0x7f0e2ba71ed0>"
                        ]
                    },
                    "execution_count": 27,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import wandb\n",
                "\n",
                "\n",
                "# Deine Run-ID, z.‚ÄØB. \"ls3jwotb\" aus der URL oder dem lokalen Log\n",
                "# Deine Run-ID, z.‚ÄØB. \"ls3jwotb\" aus der URL oder dem lokalen Log\n",
                "run_id = \"havg57rg\"\n",
                "\n",
                "# Reaktiviere den Run\n",
                "wandb.init(\n",
                "    project=\"Yolo-Training\",\n",
                "    entity=\"maats\",\n",
                "    id=run_id,\n",
                "    resume=\"allow\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "c8ab8a66",
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "\n",
                "# === Labels vorbereiten ===\n",
                "model_names = model.names  # z.‚ÄØB. {0: \"apple\", 1: \"banana\", ...}\n",
                "translation = label_translation_trained_on_10classes\n",
                "\n",
                "translated_class_labels = {\n",
                "    gt_id: model_names[pred_id]\n",
                "    for pred_id, gt_id in translation.items()\n",
                "}\n",
                "\n",
                "# === Werte aus results extrahieren ===\n",
                "class_ids = sorted(translated_class_labels.keys())  # Nur GT-Klassen, die in der √úbersetzung vorkommen\n",
                "prec_list = [results[\"classwise_precisions\"][class_ids.index(c)] for c in class_ids]\n",
                "recall_list = [results[\"classwise_recalls\"][class_ids.index(c)] for c in class_ids]\n",
                "f1_list = [results[\"classwise_f1s\"][class_ids.index(c)] for c in class_ids]\n",
                "\n",
                "# === Heatmap zeichnen ===\n",
                "metric_matrix = np.array([prec_list, recall_list, f1_list])\n",
                "metric_labels = [\"Precision\", \"Recall\", \"F1\"]\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(max(8, len(class_ids)), 4))\n",
                "sns.heatmap(\n",
                "    metric_matrix,\n",
                "    annot=True,\n",
                "    fmt=\".2f\",\n",
                "    cmap=\"YlGnBu\",\n",
                "    vmin=0.0,\n",
                "    vmax=1.0,\n",
                "    xticklabels=[translated_class_labels.get(c, str(c)) for c in class_ids],\n",
                "    yticklabels=metric_labels,\n",
                "    ax=ax\n",
                ")\n",
                "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
                "plt.title(\"Per-Class Precision / Recall / F1\")\n",
                "plt.xlabel(\"Klasse\")\n",
                "plt.ylabel(\"Metrik\")\n",
                "plt.tight_layout()\n",
                "\n",
                "wandb.log({\"new_class/per_class_metrics_heatmap\": wandb.Image(fig)})\n",
                "plt.close(fig)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "basket (3.10.12)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
